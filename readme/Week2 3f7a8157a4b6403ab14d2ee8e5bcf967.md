# Week2

202245232 이기훈

## Open FL Platform 실습

### Centralized_to_Federated - **Centralized CODE**

- Model

```python
class Net(nn.Module):

    def __init__(self) -> None:
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x: Tensor) -> Tensor:
        x = self.pool(F.relu(self.conv1(x))) #간단한 CNN층 거치고
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5) #shape 변환해서 맞춰주고
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x) # 10개 Classification
        return x
```

- 데이터셋 정보

![Untitled](Untitled%203.png)

10개의 class

60,000개의 32*32*3 사이즈 이미지

각 class당 6,000개의 데이터

**Task → Classification**

- Data Loader 코드

```python
def load_data():
    """Load CIFAR-10 (training and test set)."""
    transform = transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
    ) # 원하는 shape의 데이터 형태로 바꿔줌. (mean1,mean2,mean3),(std1,std2,std3)
    trainset = CIFAR10("./data", train=True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)
    testset = CIFAR10("./data", train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)
    num_examples = {"trainset" : len(trainset), "testset" : len(testset)}
    return trainloader, testloader, num_examples
```

- 학습하는 코드

```python
def train(net, trainloader, device, epochs):
    criterion = nn.CrossEntropyLoss() # 분류문제니 크로스엔트로피 로스
    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9) #옵티마이저는 SGD

    print(f"Training {epochs} epoch(s) w/ {len(trainloader)} batches each")

    for epoch in range(epochs):
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            images, labels = data[0].to(device), data[1].to(device)

            optimizer.zero_grad()

            outputs = net(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 100 == 99: #100번마다 log 출력
                print("[%d, %5d] loss: %.3f" % (epoch + 1, i + 1, running_loss / 2000))
                running_loss = 0.0

------output-------
Start training
Training 2 epoch(s) w/ 1563 batches each
[1,   100] loss: 0.115
[1,   200] loss: 0.115
[1,   300] loss: 0.115
[1,   400] loss: 0.115
[1,   500] loss: 0.115
[1,   600] loss: 0.115
[1,   700] loss: 0.114
[1,   800] loss: 0.114
[1,   900] loss: 0.113
[1,  1000] loss: 0.111
[1,  1100] loss: 0.108
[1,  1200] loss: 0.107
[1,  1300] loss: 0.104
[1,  1400] loss: 0.101
[1,  1500] loss: 0.099
[2,   100] loss: 0.096
[2,   200] loss: 0.095
[2,   300] loss: 0.095
[2,   400] loss: 0.094
[2,   500] loss: 0.092
[2,   600] loss: 0.091
[2,   700] loss: 0.089
[2,   800] loss: 0.090
[2,   900] loss: 0.086
[2,  1000] loss: 0.086
[2,  1100] loss: 0.087
[2,  1200] loss: 0.086
[2,  1300] loss: 0.084
[2,  1400] loss: 0.084
[2,  1500] loss: 0.085
```

- 테스트 코드

```python
def test(net, testloader, device):
    criterion = nn.CrossEntropyLoss()
    correct = 0
    total = 0
    loss = 0.0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = correct / total
    return loss, accuracy

------output-------
Evaluate model
Loss:  506.3233470916748
Accuracy:  0.4103
```

### Centralized_to_Federated - FL CODE

- server

```python
def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:
    accuracies = [num_examples * m["accuracy"] for num_examples, m in metrics]
    examples = [num_examples for num_examples, _ in metrics]

    return {"accuracy": sum(accuracies) / sum(examples)}

# Define strategy(전략)
strategy = fl.server.strategy.FedAvg(evaluate_metrics_aggregation_fn=weighted_average) #위에서 작성한 평균을 기반으로 

# Start Flower server
fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=3), #epoch와 같은의미
    strategy=strategy,
)

------output-------
INFO flower 2022-09-20 17:07:06,325 | app.py:119 | Starting Flower server, config: ServerConfig(num_rounds=3, round_timeout=None)
INFO flower 2022-09-20 17:07:06,344 | app.py:132 | Flower ECE: gRPC server running (3 rounds), SSL is disabled
INFO flower 2022-09-20 17:07:06,344 | server.py:86 | Initializing global parameters
INFO flower 2022-09-20 17:07:06,344 | server.py:270 | Requesting initial parameters from one random client
```

- client
    - Model Class → 위에서 사용한 모델과 동일한 모델
    - Train function → 위에서 사용한 학습 코드와 동일
    - Test function → 위에서 사용한 학습 코드와 동일

```python
class FlowerClient(fl.client.NumPyClient):
    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in net.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(net.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
        net.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        train(net, trainloader, epochs=1)
        return self.get_parameters(config={}), len(trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        loss, accuracy = test(net, testloader)
        return loss, len(testloader.dataset), {"accuracy": accuracy}

# Start Flower client
fl.client.start_numpy_client(server_address="127.0.0.1:8080",client=FlowerClient(),)

------Sever output-------
1번째 스크립트 켰을 때
INFO flower 2022-09-20 17:13:02,231 | server.py:274 | Received initial parameters from one random client
INFO flower 2022-09-20 17:13:02,231 | server.py:88 | Evaluating initial parameters
INFO flower 2022-09-20 17:13:02,231 | server.py:101 | FL starting

------Client output-------
INFO flower 2022-09-20 17:13:02,225 | connection.py:102 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flower 2022-09-20 17:13:02,226 | connection.py:39 | ChannelConnectivity.IDLE
DEBUG flower 2022-09-20 17:13:02,227 | connection.py:39 | ChannelConnectivity.CONNECTING
DEBUG flower 2022-09-20 17:13:02,227 | connection.py:39 | ChannelConnectivity.READY
100%|██████████| 1563/1563 [00:14<00:00, 109.41it/s]
100%|██████████| 10000/10000 [00:12<00:00, 814.70it/s]
100%|██████████| 1563/1563 [00:11<00:00, 130.42it/s]
100%|██████████| 10000/10000 [00:12<00:00, 819.37it/s]
100%|██████████| 1563/1563 [00:12<00:00, 129.07it/s]
100%|██████████| 10000/10000 [00:12<00:00, 816.42it/s]
DEBUG flower 2022-09-20 17:15:11,123 | connection.py:121 | gRPC channel closed
INFO flower 2022-09-20 17:15:11,123 | app.py:149 | Disconnect and shut down

----- Server output -----------
DEBUG flower 2022-09-20 17:13:55,899 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)
DEBUG flower 2022-09-20 17:14:10,196 | server.py:229 | fit_round 1 received 2 results and 0 failures
WARNING flower 2022-09-20 17:14:10,206 | fedavg.py:243 | No fit_metrics_aggregation_fn provided
DEBUG flower 2022-09-20 17:14:10,206 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)
DEBUG flower 2022-09-20 17:14:22,493 | server.py:179 | evaluate_round 1 received 2 results and 0 failures
DEBUG flower 2022-09-20 17:14:22,493 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)
DEBUG flower 2022-09-20 17:14:34,491 | server.py:229 | fit_round 2 received 2 results and 0 failures
DEBUG flower 2022-09-20 17:14:34,497 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)
DEBUG flower 2022-09-20 17:14:46,713 | server.py:179 | evaluate_round 2 received 2 results and 0 failures
DEBUG flower 2022-09-20 17:14:46,713 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)
DEBUG flower 2022-09-20 17:14:58,856 | server.py:229 | fit_round 3 received 2 results and 0 failures
DEBUG flower 2022-09-20 17:14:58,862 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)
DEBUG flower 2022-09-20 17:15:11,120 | server.py:179 | evaluate_round 3 received 2 results and 0 failures
INFO flower 2022-09-20 17:15:11,120 | server.py:144 | FL finished in 128.88882779999994
INFO flower 2022-09-20 17:15:11,120 | app.py:180 | app_fit: losses_distributed [(1, 1.9489716291427612), (2, 1.604902982711792), (3, 1.483713984489441)]
INFO flower 2022-09-20 17:15:11,120 | app.py:181 | app_fit: metrics_distributed {'accuracy': [(1, 0.2865), (2, 0.4088), (3, 0.4593)]}
INFO flower 2022-09-20 17:15:11,120 | app.py:182 | app_fit: losses_centralized []
INFO flower 2022-09-20 17:15:11,120 | app.py:183 | app_fit: metrics_centralized {}
```

정확도가 `{'accuracy': [(1, 0.2865), (2, 0.4088), (3, 0.4593)]}` 로 나왔다.

### (FE)MNIST Dataset - **Centralized CODE**

- Dataset 정보

![Untitled](Untitled%204.png)

- Model, Dataset class code

```python
class FemnistDataset(Dataset): #데이터셋 받아오는 class
    def __init__(self, dataset, transform):
        self.x = dataset['x']
        self.y = dataset['y']
        self.transform = transform

    def __getitem__(self, index):
        input_data = np.array(self.x[index]).reshape(28,28,1)
        if self.transform:
            input_data = self.transform(input_data)
        target_data = self.y[index]
        return input_data, target_data

    def __len__(self):
        return len(self.y)

class femnist_network(nn.Module):
    def __init__(self) -> None:
        super(femnist_network, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding=2)
        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)
        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))
        self.linear1 = nn.Linear(7*7*64, 2048)
        self.linear2 = nn.Linear(2048, 62)

    def forward(self, x:torch.Tensor) -> torch.Tensor: #간단한 CNN 층
        x = torch.relu(self.conv1(x))
        x = self.maxpool1(x)
        x = torch.relu(self.conv2(x))
        x = self.maxpool2(x)
        x = torch.flatten(x, start_dim=1)
        x = torch.relu((self.linear1(x)))
        x = self.linear2(x) #62개의 Class중에서 택1
        return x
```

- Client code

```python
class Client(fl.client.NumPyClient):
        def get_parameters(self, config):
            return [val.cpu().numpy() for _, val in net.state_dict().items()]

        def set_parameters(self, parameters):
            params_dict = zip(net.state_dict().keys(), parameters)
            state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
            net.load_state_dict(state_dict, strict=True)

        def fit(self, parameters, config):
            self.set_parameters(parameters)
            trainloader, _ = load_data()
            train(net, trainloader, epochs=20)
            return self.get_parameters(config={}), len(trainloader.dataset), {}

        def evaluate(self, parameters, config):
            self.set_parameters(parameters)
            _, testloader = load_data()
            loss, accuracy = test(net, testloader)
            return float(loss), len(testloader.dataset), {"accuracy": accuracy}
```

- Main code

```python
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    net = femnist_network().to(DEVICE)

    def load_data():
        transform = transforms.Compose([transforms.ToTensor()])

        number = random.randint(0, 35)
        if number == 35:
            subject_number = random.randint(0, 96)
        else:
            subject_number = random.randint(0, 99)

        print('number : {}, subject number : {}'.format(number, subject_number))
        with open("../data/train/all_data_"+str(number)+"_niid_0_keep_0_train_9.json","r") as f:
            train_json = json.load(f)
        with open("../data/test/all_data_"+str(number)+"_niid_0_keep_0_test_9.json","r") as f:
            test_json = json.load(f)
        train_user = train_json['users'][subject_number] #(1)
        train_data = train_json['user_data'][train_user] #(2)
        test_user = test_json['users'][subject_number] 
        test_data = test_json['user_data'][test_user]
        trainset = FemnistDataset(train_data, transform)
        testset = FemnistDataset(test_data, transform)
        trainloader = DataLoader(trainset, batch_size=64, shuffle=True)
        testloader = DataLoader(testset, batch_size=64)
        return trainloader, testloader
```

![Untitled](Untitled%205.png)

(1) User 데이터 같은 경우에는 100개가 있고,

![Untitled](Untitled%206.png)

(2) 각 User에 대한 User_data 값이 들어있음.

똑같이 학습하고 결과 뽑으면 

```python
-----Client Output-----
<Process name='Process-1' parent=35500 initial>
<Process name='Process-2' parent=35500 initial>
INFO flower 2022-09-20 18:02:08,658 | connection.py:102 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flower 2022-09-20 18:02:08,665 | connection.py:39 | ChannelConnectivity.CONNECTING
DEBUG flower 2022-09-20 18:02:08,688 | connection.py:39 | ChannelConnectivity.IDLE
DEBUG flower 2022-09-20 18:02:08,689 | connection.py:39 | ChannelConnectivity.READY
number : 30, subject number : 8
number : 3, subject number : 98
number : 19, subject number : 16
...(생략)
number : 6, subject number : 61
number : 12, subject number : 34
number : 14, subject number : 63
DEBUG flower 2022-09-20 18:05:47,817 | connection.py:121 | gRPC channel closed
DEBUG flower 2022-09-20 18:05:47,817 | connection.py:121 | gRPC channel closed
DEBUG flower 2022-09-20 18:05:47,817 | connection.py:121 | gRPC channel closed
INFO flower 2022-09-20 18:05:47,817 | app.py:149 | Disconnect and shut down
INFO flower 2022-09-20 18:05:47,817 | app.py:149 | Disconnect and shut down
INFO flower 2022-09-20 18:05:47,817 | app.py:149 | Disconnect and shut down

-----Server Output-----
accuracy : 0.09230769230769231
DEBUG flower 2022-09-20 18:05:30,595 | server.py:229 | fit_round 19 received 3 results and 0 failures
DEBUG flower 2022-09-20 18:05:30,673 | server.py:165 | evaluate_round 19: strategy sampled 3 clients (out of 3)
accuracy : 0.03389830508474576
DEBUG flower 2022-09-20 18:05:36,392 | server.py:179 | evaluate_round 19 received 3 results and 0 failures
DEBUG flower 2022-09-20 18:05:36,392 | server.py:215 | fit_round 20: strategy sampled 3 clients (out of 3)
DEBUG flower 2022-09-20 18:05:42,766 | server.py:229 | fit_round 20 received 3 results and 0 failures
DEBUG flower 2022-09-20 18:05:42,843 | server.py:165 | evaluate_round 20: strategy sampled 3 clients (out of 3)
DEBUG flower 2022-09-20 18:05:47,809 | server.py:179 | evaluate_round 20 received 3 results and 0 failures
INFO flower 2022-09-20 18:05:47,809 | server.py:144 | FL finished in 219.0097775
INFO flower 2022-09-20 18:05:47,812 | app.py:180 | app_fit: losses_distributed [(1, 4.120197376647553), (2, 4.105217112027682), (3, 4.082891008433173), (4, 4.069174562181745), (5, 4.049187129398562), (6, 4.0418499605996265), (7, 4.030423994305767), (8, 4.028253670894738), (9, 3.981293412355276), (10, 3.844920181311094), (11, 3.906408343464136), (12, 3.717146635055542), (13, 3.857051975124485), (14, 3.7693488317377426), (15, 3.8815427612751088), (16, 3.7477396627267203), (17, 3.8767310937245685), (18, 3.614330346767719), (19, 3.597563177852307), (20, 3.7063026229540506)]
INFO flower 2022-09-20 18:05:47,812 | app.py:181 | app_fit: metrics_distributed {'accuracy': [(1, 0.025974025974025976), (2, 0.046153846153846156), (3, 0.08823529411764706), (4, 0.1038961038961039), (5, 0.07547169811320754), (6, 0.07142857142857142), (7, 0.0379746835443038), (8, 0.07575757575757576), (9, 0.019230769230769232), (10, 0.19230769230769232), (11, 0.0625), (12, 0.07547169811320754), (13, 0.054945054945054944), (14, 0.014705882352941176), (15, 0.031914893617021274), (16, 0.06944444444444445), (17, 0.08333333333333333), (18, 0.09230769230769231), (19, 0.03389830508474576), (20, 0.041666666666666664)]}
INFO flower 2022-09-20 18:05:47,812 | app.py:182 | app_fit: losses_centralized []
INFO flower 2022-09-20 18:05:47,812 | app.py:183 | app_fit: metrics_centralized {}
accuracy : 0.041666666666666664
```

랜덤으로 값을 뽑고 학습을 진행함.

### 추가적인 학습

**연합 학습(FL: Federated Learning)**
다수의 로컬 클라이언트와 하나의 중앙 서버가 협력하여 데이터가 탈중앙화된 상황에서 글로벌 모델을 학습하는 기술

즉, 분산학습과는 다름. 데이터 유출 없이 학습 가능한 **완전 탈중앙 학습**임.

![Untitled](Untitled%207.png)

기존 코드 → 연합학습 코드로 변경

```python
#server.py

import flwr as fl

if __name__ == "__main__":
    fl.server.start_server(server_address="0.0.0.0:8080", config=fl.server.ServerConfig(num_rounds=3))
```

```python
#Client.py

from collections import OrderedDict
from typing import Dict, List, Tuple
import numpy as np
import torch
import cifar
import flwr as fl

DEVICE: str = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class Client(fl.client.NumPyClient):
    def __init__(
        self,
        model: User_model, # 모델 넣어주기
        trainloader: torch.utils.data.DataLoader,
        testloader: torch.utils.data.DataLoader,
        num_examples: Dict,
    ) -> None:
        self.model = model
        self.trainloader = trainloader
        self.testloader = testloader
        self.num_examples = num_examples

    def get_parameters(self, config) -> List[np.ndarray]:
        # Return model parameters as a list of NumPy ndarrays
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters: List[np.ndarray]) -> None:
        # Set model parameters from a list of NumPy ndarrays
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
        self.model.load_state_dict(state_dict, strict=True)

    def fit(
        self, parameters: List[np.ndarray], config: Dict[str, str]
    ) -> Tuple[List[np.ndarray], int, Dict]:
        # Set model parameters, train model, return updated model parameters
        self.set_parameters(parameters)
        cifar.train(self.model, self.trainloader, epochs=1, device=DEVICE)
        return self.get_parameters(config={}), self.num_examples["trainset"], {}

    def evaluate(
        self, parameters: List[np.ndarray], config: Dict[str, str]
    ) -> Tuple[float, int, Dict]:
        # Set model parameters, evaluate model on local test dataset, return result
        self.set_parameters(parameters)
        loss, accuracy = cifar.test(self.model, self.testloader, device=DEVICE)
        return float(loss), self.num_examples["testset"], {"accuracy": float(accuracy)}

def main() :
    model = User_Model()
    model.to(DEVICE)
    trainloader, testloader, num_examples = load_data()

    # Start client
    client = Client(model, trainloader, testloader, num_examples)
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client)

if __name__ == "__main__":
    main()
```

flower 공식 홈페이지의 Docs를 보니 이렇게 하면 기존 코드에서 연합학습으로 돌릴 수 있다고 함.

그러나, 지나치게 단순화 되어있고 현실적이지 않다고 쓰여져 있어서 추가적으로 공부를 한 다음에 적용시켜야 할듯함.